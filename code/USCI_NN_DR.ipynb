{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WBCD Dataset Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "from textwrap import wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import more Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 25\n",
    "\n",
    "df = pd.read_csv('../datasets/us_income/adult-train.csv', delimiter=',', quotechar='\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe without Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Printing dataframe head (without any preprocessing)....\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(' <=50K', 0, inplace=True)\n",
    "df.replace(' >50K', 1, inplace=True)\n",
    "\n",
    "df = pd.get_dummies(df)\n",
    "# print(df.head(5))\n",
    "\n",
    "X = df.loc[:, df.columns != 'Income']\n",
    "y = df['Income']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df=(df-df.mean())/df.std()\n",
    "print(normalized_df.head(10))\n",
    "# X_normalized = normalized_df.loc[:, normalized_df.columns != 'Income']\n",
    "X = normalized_df.loc[:, normalized_df.columns != 'Income']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split into 30%  training data, 70% testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.30, random_state=RANDOM_SEED)\n",
    "\n",
    "\n",
    "# # Apply scaling. Large values of certain features undesireable for NN\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNClassifier\n",
    "train_size = len(X_train)\n",
    "offsets = range(int(0.1 * train_size), int(train_size), int(0.1 * train_size))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_err = [0] * len(offsets)\n",
    "test_err = [0] * len(offsets)\n",
    "\n",
    "print('training_set_max_size:', train_size, '\\n')\n",
    "\n",
    "activation_functions = ['relu', 'logistic', 'tanh']\n",
    "\n",
    "for activation in activation_functions:\n",
    "    for i, o in enumerate(offsets):\n",
    "        print('activation: ' + activation)\n",
    "        print('learning a neural net with training_set_size=' + str(o))\n",
    "        print('getting data'),\n",
    "        X_train_temp = X_train[:o].copy()\n",
    "        y_train_temp = y_train[:o].copy()\n",
    "        X_test_temp = X_test[:o].copy()\n",
    "        y_test_temp = y_test[:o].copy()\n",
    "        print('building net'),\n",
    "        mlp = MLPClassifier(activation=activation, alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "           beta_2=0.999, epsilon=1e-08,\n",
    "           # hidden_layer_sizes=(50, 25, 12),\n",
    "           hidden_layer_sizes=(13, 13, 13),\n",
    "           learning_rate='constant',\n",
    "           learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
    "           nesterovs_momentum=True, random_state=None,\n",
    "           shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1)\n",
    "        print('training'),\n",
    "        mlp.fit(X_train,y_train)\n",
    "        print('validating')\n",
    "        train_err[i] = mean_squared_error(y_train_temp,\n",
    "                    mlp.predict(X_train_temp))\n",
    "        test_err[i] = mean_squared_error(y_test_temp,\n",
    "                    mlp.predict(X_test_temp))\n",
    "\n",
    "        # print(classification_report(y_train, mlp.predict(X_train)))\n",
    "        # print(classification_report(y_test, mlp.predict(X_test)))\n",
    "\n",
    "        print('train_err: ' + str(train_err[i]))\n",
    "        print('test_err: ' + str(test_err[i]))\n",
    "        print('---')\n",
    "\n",
    "    # Plot results\n",
    "    print('plotting results')\n",
    "    plt.figure()\n",
    "    title = 'USCI Neural Nets: Performance x Training Set Size using Activation ' + activation\n",
    "    plt.title('\\n'.join(wrap(title,60)))\n",
    "    # plt.subplots_adjust(top=0.85)\n",
    "    plt.plot(offsets, test_err, '-', label='test error')\n",
    "    plt.plot(offsets, train_err, '-', label='train error')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('Mean Square Error')\n",
    "    filename = 'wbcd' + activation + '_PerformancexTrainingSetSize.png'\n",
    "    plt.show()\n",
    "#     plt.savefig('plots/WBCD/NN/' + filename)\n",
    "    print('plot complete')\n",
    "    ### ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
