{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from textwrap import wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_DEPTH_LIMIT = 20\n",
    "RANDOM_SEED = 25\n",
    "\n",
    "\n",
    "columns = ['Radius','Texture','Perimeter','Area','Smoothness','Compactness',\n",
    "           'Concavity','Concave_Points','Symmetry','Fractal_Dimension',\n",
    "           'Malignant/Benign']\n",
    "\n",
    "# Read CSV file into pandas df\n",
    "df = pd.read_csv('../datasets/breast_cancer/breast-cancer-wisconsin.csv',\n",
    "                 delimiter=',', quotechar='\"', names=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing, Training Testing Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing df.shape after handling categorical data:  (683, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.5/site-packages/ipykernel_launcher.py:20: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/home/saksham/.local/lib/python3.5/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/saksham/.local/lib/python3.5/site-packages/ipykernel_launcher.py:35: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "/home/saksham/.local/lib/python3.5/site-packages/ipykernel_launcher.py:36: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "# Shuffle\n",
    "df = shuffle(df, random_state=RANDOM_SEED)\n",
    "\n",
    "# DROP USELESS ROWS AND COLUMNS\n",
    "df.dropna(inplace=True)\n",
    "cols = [0]\n",
    "# Drop ID column (it's not attribute or target)\n",
    "df.drop(df.columns[cols],axis=1,inplace=True)\n",
    "# Drop all data points with missing variables  (denoted by '?' entry)\n",
    "nostrings_row_list = [x.isdigit() for x in df.iloc[:,5]]\n",
    "df = df[nostrings_row_list]\n",
    "\n",
    "\n",
    "# Handle categorical data\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "print(\"Printing df.shape after handling categorical data: \", df.shape)\n",
    "\n",
    "# Split data into X and y vectors\n",
    "X = df.ix[:, df.columns != 'Malignant/Benign']\n",
    "y = df['Malignant/Benign']\n",
    "\n",
    "# Change 2 -> 0 (benign) and 4 -> 1 (malignant)\n",
    "y.replace(2, 0, inplace=True)\n",
    "y.replace(4, 1, inplace=True)\n",
    "\n",
    "# Split into 30%  training data, 70% testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.30, random_state=RANDOM_SEED)\n",
    "\n",
    "\n",
    "# Apply scaling. Large values of certain features undesireable for NN\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Neural Network (without dimensionality reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set_max_size: 478 \n",
      "\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=47\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.0\n",
      "test_err: 0.0851063829787234\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=94\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.0\n",
      "test_err: 0.0425531914893617\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=141\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.0\n",
      "test_err: 0.07092198581560284\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=188\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.0\n",
      "test_err: 0.06382978723404255\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=235\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.00425531914893617\n",
      "test_err: 0.03902439024390244\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=282\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.0035460992907801418\n",
      "test_err: 0.04878048780487805\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=329\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.00303951367781155\n",
      "test_err: 0.06341463414634146\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=376\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.0\n",
      "test_err: 0.04878048780487805\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=423\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.002364066193853428\n",
      "test_err: 0.03902439024390244\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=470\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.002127659574468085\n",
      "test_err: 0.04878048780487805\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=47\n",
      "getting data\n",
      "building net\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n",
      "train_err: 0.02127659574468085\n",
      "test_err: 0.02127659574468085\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=94\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.031914893617021274\n",
      "test_err: 0.0425531914893617\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=141\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.02127659574468085\n",
      "test_err: 0.0425531914893617\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=188\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.015957446808510637\n",
      "test_err: 0.03723404255319149\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=235\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.01276595744680851\n",
      "test_err: 0.03414634146341464\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=282\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.01773049645390071\n",
      "test_err: 0.03414634146341464\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=329\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.015197568389057751\n",
      "test_err: 0.03414634146341464\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=376\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.02127659574468085\n",
      "test_err: 0.03414634146341464\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=423\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.016548463356973995\n",
      "test_err: 0.03414634146341464\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=470\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.014893617021276596\n",
      "test_err: 0.03414634146341464\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=47\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.0\n",
      "test_err: 0.0425531914893617\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=94\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.0\n",
      "test_err: 0.0851063829787234\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=141\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.0\n",
      "test_err: 0.05673758865248227\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=188\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.0\n",
      "test_err: 0.07446808510638298\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=235\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.0\n",
      "test_err: 0.04878048780487805\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=282\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.0035460992907801418\n",
      "test_err: 0.03902439024390244\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=329\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.0\n",
      "test_err: 0.07317073170731707\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=376\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.0\n",
      "test_err: 0.05365853658536585\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=423\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.004728132387706856\n",
      "test_err: 0.04878048780487805\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=470\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err: 0.00425531914893617\n",
      "test_err: 0.06341463414634146\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# NNClassifier\n",
    "train_size = len(X_train)\n",
    "offsets = range(int(0.1 * train_size), int(train_size), int(0.1 * train_size))\n",
    "\n",
    "\n",
    "train_err = [0] * len(offsets)\n",
    "test_err = [0] * len(offsets)\n",
    "\n",
    "print('training_set_max_size:', train_size, '\\n')\n",
    "\n",
    "activation_functions = ['relu', 'logistic', 'tanh']\n",
    "\n",
    "for activation in activation_functions:\n",
    "    for i, o in enumerate(offsets):\n",
    "        print('activation: ' + activation)\n",
    "        print('learning a neural net with training_set_size=' + str(o))\n",
    "        print('getting data'),\n",
    "        X_train_temp = X_train[:o].copy()\n",
    "        y_train_temp = y_train[:o].copy()\n",
    "        X_test_temp = X_test[:o].copy()\n",
    "        y_test_temp = y_test[:o].copy()\n",
    "        print('building net'),\n",
    "        mlp = MLPClassifier(activation=activation, alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "           beta_2=0.999, epsilon=1e-08,\n",
    "           # hidden_layer_sizes=(50, 25, 12),\n",
    "           hidden_layer_sizes=(13, 13, 13),\n",
    "           learning_rate='constant',\n",
    "           learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
    "           nesterovs_momentum=True, random_state=None,\n",
    "           shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1)\n",
    "        print('training'),\n",
    "        mlp.fit(X_train,y_train)\n",
    "        print('validating')\n",
    "        train_err[i] = mean_squared_error(y_train_temp,\n",
    "                    mlp.predict(X_train_temp))\n",
    "        test_err[i] = mean_squared_error(y_test_temp,\n",
    "                    mlp.predict(X_test_temp))\n",
    "\n",
    "        # print(classification_report(y_train, mlp.predict(X_train)))\n",
    "        # print(classification_report(y_test, mlp.predict(X_test)))\n",
    "\n",
    "        print('train_err: ' + str(train_err[i]))\n",
    "        print('test_err: ' + str(test_err[i]))\n",
    "        print('---')\n",
    "\n",
    "#     # Plot results\n",
    "#     print 'plotting results'\n",
    "#     plt.figure()\n",
    "#     title = 'WBCD Neural Nets: Performance x Training Set Size using Activation ' + activation\n",
    "#     plt.title('\\n'.join(wrap(title,60)))\n",
    "#     # plt.subplots_adjust(top=0.85)\n",
    "#     plt.plot(offsets, test_err, '-', label='test error')\n",
    "#     plt.plot(offsets, train_err, '-', label='train error')\n",
    "#     plt.legend()\n",
    "#     plt.xlabel('Training Set Size')\n",
    "#     plt.ylabel('Mean Square Error')\n",
    "#     filename = 'wbcd' + activation + '_PerformancexTrainingSetSize.png'\n",
    "#     plt.savefig('plots/WBCD/NN/' + filename)\n",
    "#     print 'plot complete'\n",
    "#     ### ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction Before Training NN  (comment sections out as needed for experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6943919  0.07395622 0.05952882 0.04877117 0.04402469]\n",
      "Printing X_dim_reduced...\n",
      "[[-4.7434279  -1.08196618 -0.07378399 -0.30841019  0.62702353]\n",
      " [-4.09575951 -1.83182431  0.10855599 -0.7258196   0.21829327]\n",
      " [10.43986046  0.34244016 -2.17452341 -1.9924894  -0.35400733]\n",
      " ...\n",
      " [-4.41658787 -1.94511079  0.02546406 -0.65961409  0.70202035]\n",
      " [-4.24209664  1.91942415 -0.31639195  1.67748745  0.44883327]\n",
      " [10.67359358  3.3460804   2.38923349  1.64026588  2.39851963]]\n"
     ]
    }
   ],
   "source": [
    "# USING PCA \n",
    "\n",
    "n_comp = 5 \n",
    "pca = PCA(n_components=n_comp)\n",
    "X_dim_reduced = pca.fit(X).transform(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "print(\"Printing X_dim_reduced...\")\n",
    "print(X_dim_reduced)\n",
    "\n",
    "\n",
    "# Split into 30%  training data, 70% testing data\n",
    "X_train_dr, X_test_dr, y_train_dr, y_test_dr = train_test_split(X_dim_reduced, y,\n",
    "                                                    test_size=0.30, random_state=RANDOM_SEED)\n",
    "\n",
    "\n",
    "# Apply scaling. Large values of certain features undesireable for NN\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_dr)\n",
    "X_train_dr = scaler.transform(X_train_dr)\n",
    "X_test_dr = scaler.transform(X_train_dr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run clustering, append to the features for NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c726dc05a54d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf_kMeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mclf_kMeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_kMeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KMeans' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "clf_kMeans = KMeans(n_clusters=2, random_state=0)\n",
    "clf_kMeans.fit(X)\n",
    "\n",
    "error = mean_squared_error(y, clf_kMeans.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic NN (Get it running with Dimnesionality Reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n",
      "Training error:  0.027659574468085105\n",
      "Testing error:  0.02926829268292683\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "           beta_2=0.999, epsilon=1e-08,\n",
    "           # hidden_layer_sizes=(50, 25, 12),\n",
    "           hidden_layer_sizes=(13, 13, 13),\n",
    "           learning_rate='constant',\n",
    "           learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
    "           nesterovs_momentum=True, random_state=None,\n",
    "           shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1)\n",
    "\n",
    "mlp.fit(X_train_dr,y_train_dr)\n",
    "\n",
    "print('validating')\n",
    "trainingError_debug = mean_squared_error(y_train_temp_dr,\n",
    "            mlp.predict(X_train_temp_dr))\n",
    "testingError_debug = mean_squared_error(y_test_temp_dr,\n",
    "            mlp.predict(X_test_temp_dr))\n",
    "\n",
    "print(\"Training error: \", trainingError_debug)\n",
    "print(\"Testing error: \", testingError_debug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Neural Network (after dimensionality reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set_max_size: 478 \n",
      "\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=47\n",
      "getting data\n",
      "building net\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n",
      "train_err_dr: 0.0\n",
      "test_err_dr: 0.0425531914893617\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=94\n",
      "getting data\n",
      "building net\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n",
      "train_err_dr: 0.010638297872340425\n",
      "test_err_dr: 0.06382978723404255\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=141\n",
      "getting data\n",
      "building net\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n",
      "train_err_dr: 0.014184397163120567\n",
      "test_err_dr: 0.05673758865248227\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=188\n",
      "getting data\n",
      "building net\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n",
      "train_err_dr: 0.010638297872340425\n",
      "test_err_dr: 0.05319148936170213\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=235\n",
      "getting data\n",
      "building net\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n",
      "train_err_dr: 0.01702127659574468\n",
      "test_err_dr: 0.03902439024390244\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=282\n",
      "getting data\n",
      "building net\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n",
      "train_err_dr: 0.02127659574468085\n",
      "test_err_dr: 0.03414634146341464\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=329\n",
      "getting data\n",
      "building net\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n",
      "train_err_dr: 0.015197568389057751\n",
      "test_err_dr: 0.03902439024390244\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=376\n",
      "getting data\n",
      "building net\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n",
      "train_err_dr: 0.026595744680851064\n",
      "test_err_dr: 0.03902439024390244\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=423\n",
      "getting data\n",
      "building net\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n",
      "train_err_dr: 0.018912529550827423\n",
      "test_err_dr: 0.04390243902439024\n",
      "---\n",
      "activation: relu\n",
      "learning a neural net with training_set_size=470\n",
      "getting data\n",
      "building net\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n",
      "train_err_dr: 0.01702127659574468\n",
      "test_err_dr: 0.03902439024390244\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=47\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.02127659574468085\n",
      "test_err_dr: 0.02127659574468085\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=94\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.031914893617021274\n",
      "test_err_dr: 0.05319148936170213\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=141\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.028368794326241134\n",
      "test_err_dr: 0.0425531914893617\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=188\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.34574468085106386\n",
      "test_err_dr: 0.3670212765957447\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=235\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.02553191489361702\n",
      "test_err_dr: 0.02926829268292683\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=282\n",
      "getting data\n",
      "building net\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n",
      "train_err_dr: 0.028368794326241134\n",
      "test_err_dr: 0.02926829268292683\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=329\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.03343465045592705\n",
      "test_err_dr: 0.03414634146341464\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=376\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.031914893617021274\n",
      "test_err_dr: 0.03414634146341464\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=423\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.026004728132387706\n",
      "test_err_dr: 0.02926829268292683\n",
      "---\n",
      "activation: logistic\n",
      "learning a neural net with training_set_size=470\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.02553191489361702\n",
      "test_err_dr: 0.02926829268292683\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=47\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.02127659574468085\n",
      "test_err_dr: 0.0425531914893617\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=94\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.031914893617021274\n",
      "test_err_dr: 0.06382978723404255\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=141\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.028368794326241134\n",
      "test_err_dr: 0.0425531914893617\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=188\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.026595744680851064\n",
      "test_err_dr: 0.03723404255319149\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=235\n",
      "getting data\n",
      "building net\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n",
      "train_err_dr: 0.01276595744680851\n",
      "test_err_dr: 0.02926829268292683\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=282\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.031914893617021274\n",
      "test_err_dr: 0.02926829268292683\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=329\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.03343465045592705\n",
      "test_err_dr: 0.03414634146341464\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=376\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.034574468085106384\n",
      "test_err_dr: 0.03902439024390244\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=423\n",
      "getting data\n",
      "building net\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n",
      "train_err_dr: 0.026004728132387706\n",
      "test_err_dr: 0.03414634146341464\n",
      "---\n",
      "activation: tanh\n",
      "learning a neural net with training_set_size=470\n",
      "getting data\n",
      "building net\n",
      "training\n",
      "validating\n",
      "train_err_dr: 0.031914893617021274\n",
      "test_err_dr: 0.02926829268292683\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# NNClassifier\n",
    "train_size = len(X_train_dr)\n",
    "offsets = range(int(0.1 * train_size), int(train_size), int(0.1 * train_size))\n",
    "\n",
    "\n",
    "train_err_dr = [0] * len(offsets)\n",
    "test_err_dr = [0] * len(offsets)\n",
    "\n",
    "print('training_set_max_size:', train_size, '\\n')\n",
    "\n",
    "activation_functions = ['relu', 'logistic', 'tanh']\n",
    "\n",
    "for activation in activation_functions:\n",
    "    for i, o in enumerate(offsets):\n",
    "        print('activation: ' + activation)\n",
    "        print('learning a neural net with training_set_size=' + str(o))\n",
    "        print('getting data'),\n",
    "        X_train_temp_dr = X_train_dr[:o].copy()\n",
    "        y_train_temp_dr = y_train_dr[:o].copy()\n",
    "        X_test_temp_dr = X_test_dr[:o].copy()\n",
    "        y_test_temp_dr = y_test_dr[:o].copy()\n",
    "        print('building net'),\n",
    "        mlp = MLPClassifier(activation=activation, alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "           beta_2=0.999, epsilon=1e-08,\n",
    "           hidden_layer_sizes=(13, 13, 13),\n",
    "           learning_rate='constant',\n",
    "           learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
    "           nesterovs_momentum=True, random_state=None,\n",
    "           shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1)\n",
    "        print('training'),\n",
    "        mlp.fit(X_train_dr,y_train_dr)\n",
    "        print('validating')\n",
    "        train_err_dr[i] = mean_squared_error(y_train_temp_dr,\n",
    "                    mlp.predict(X_train_temp_dr))\n",
    "        test_err_dr[i] = mean_squared_error(y_test_temp_dr,\n",
    "                    mlp.predict(X_test_temp_dr))\n",
    "\n",
    "        # print(classification_report(y_train, mlp.predict(X_train)))\n",
    "        # print(classification_report(y_test, mlp.predict(X_test)))\n",
    "\n",
    "        print('train_err_dr: ' + str(train_err_dr[i]))\n",
    "        print('test_err_dr: ' + str(test_err_dr[i]))\n",
    "        print('---')\n",
    "\n",
    "#     # Plot results\n",
    "#     print 'plotting results'\n",
    "#     plt.figure()\n",
    "#     title = 'WBCD Neural Nets: Performance x Training Set Size using Activation ' + activation\n",
    "#     plt.title('\\n'.join(wrap(title,60)))\n",
    "#     # plt.subplots_adjust(top=0.85)\n",
    "#     plt.plot(offsets, test_err, '-', label='test error')\n",
    "#     plt.plot(offsets, train_err, '-', label='train error')\n",
    "#     plt.legend()\n",
    "#     plt.xlabel('Training Set Size')\n",
    "#     plt.ylabel('Mean Square Error')\n",
    "#     filename = 'wbcd' + activation + '_PerformancexTrainingSetSize.png'\n",
    "#     plt.savefig('plots/WBCD/NN/' + filename)\n",
    "#     print 'plot complete'\n",
    "#     ### ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
